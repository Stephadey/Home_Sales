{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f9d778d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import findspark and initialise. \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, round\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# 1. Read in the home_sales_revised.csv data into a Spark DataFrame.\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(\"home_sales_revised.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 2. Create a temporary table called home_sales.\n",
    "df.createOrReplaceTempView(\"home_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5f64ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|year|avg_price|\n",
      "+----+---------+\n",
      "|2019| 300263.7|\n",
      "|2020|298353.78|\n",
      "|2021|301819.44|\n",
      "|2022|296363.88|\n",
      "+----+---------+\n",
      "\n",
      "+----+---------+\n",
      "|year|avg_price|\n",
      "+----+---------+\n",
      "|2019| 300263.7|\n",
      "|2020|298353.78|\n",
      "|2021|301819.44|\n",
      "|2022|296363.88|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. What is the average price for a four-bedroom house sold for each year? Round off your answer to two decimal places.\n",
    "avg_price_4_bedroom = spark.sql(\"\"\"\n",
    "    SELECT year(date) AS year, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 4\n",
    "    GROUP BY year(date)\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "avg_price_4_bedroom.show()\n",
    "avg_price_4_bedroom.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "39e2a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2010|292859.62|\n",
      "|      2011|291117.47|\n",
      "|      2012|293683.19|\n",
      "|      2013|295962.27|\n",
      "|      2014|290852.27|\n",
      "|      2015| 288770.3|\n",
      "|      2016|290555.07|\n",
      "|      2017|292676.79|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. What is the average price of a home for each year the home was built, that has three bedrooms and three bathrooms? Round off your answer to two decimal places.\n",
    "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
    "avg_price_3_bed_3_bath = spark.sql(\"\"\"\n",
    "    SELECT date_built AS year_built, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 3 AND bathrooms = 3\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built\n",
    "\"\"\")\n",
    "avg_price_3_bed_3_bath.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2bae55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|year_built|avg_price|\n",
      "+----------+---------+\n",
      "|      2010|285010.22|\n",
      "|      2011|276553.81|\n",
      "|      2012|307539.97|\n",
      "|      2013|303676.79|\n",
      "|      2014|298264.72|\n",
      "|      2015|297609.97|\n",
      "|      2016| 293965.1|\n",
      "|      2017|280317.58|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. What is the average price of a home for each year the home was built, that has three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet? Round off your answer to two decimal places.\n",
    "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
    "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
    "avg_price_specific = spark.sql(\"\"\"\n",
    "    SELECT date_built AS year_built, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
    "    GROUP BY date_built\n",
    "    ORDER BY date_built\n",
    "\"\"\")\n",
    "avg_price_specific.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "65768ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|view| avg_price|\n",
      "+----+----------+\n",
      "|  91|1137372.73|\n",
      "|  97|1129040.15|\n",
      "|  84|1117233.13|\n",
      "|  75|1114042.94|\n",
      "|  89|1107839.15|\n",
      "|  78|1080649.37|\n",
      "|  77|1076205.56|\n",
      "|  87| 1072285.2|\n",
      "|  86|1070444.25|\n",
      "|  82| 1063498.0|\n",
      "|  90|1062654.16|\n",
      "|  99|1061201.42|\n",
      "|  76|1058802.78|\n",
      "|  85|1056336.74|\n",
      "|  95| 1054325.6|\n",
      "|  98|1053739.33|\n",
      "|  81|1053472.79|\n",
      "|  83|1033965.93|\n",
      "|  94| 1033536.2|\n",
      "|  88|1031719.35|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Uncached runtime: 0.24407029151916504 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. What is the average price of a home per \"view\" rating having an average home price greater than or equal to $350,000? Determine the runtime for this query, and round off your answer to two decimal places.\n",
    "# having an average home price greater than or equal to $350,000? Order by descending view rating. \n",
    "# Although this is a small dataset, determine the run time for this query.\n",
    "start_time = time.time()\n",
    "avg_price_view = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING AVG(price) >= 350000\n",
    "    ORDER BY avg_price DESC\n",
    "\"\"\")\n",
    "avg_price_view.show()\n",
    "uncached_runtime = time.time() - start_time\n",
    "print(f\"Uncached runtime: {uncached_runtime} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a345b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'home_sales' cached? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Cache the temporary table home_sales.\n",
    "spark.catalog.cacheTable(\"home_sales\")\n",
    "\n",
    "# 8. Check if the table is cached.\n",
    "print(f\"Is 'home_sales' cached? {spark.catalog.isCached('home_sales')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "706d7ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|view| avg_price|\n",
      "+----+----------+\n",
      "|  91|1137372.73|\n",
      "|  97|1129040.15|\n",
      "|  84|1117233.13|\n",
      "|  75|1114042.94|\n",
      "|  89|1107839.15|\n",
      "|  78|1080649.37|\n",
      "|  77|1076205.56|\n",
      "|  87| 1072285.2|\n",
      "|  86|1070444.25|\n",
      "|  82| 1063498.0|\n",
      "|  90|1062654.16|\n",
      "|  99|1061201.42|\n",
      "|  76|1058802.78|\n",
      "|  85|1056336.74|\n",
      "|  95| 1054325.6|\n",
      "|  98|1053739.33|\n",
      "|  81|1053472.79|\n",
      "|  83|1033965.93|\n",
      "|  94| 1033536.2|\n",
      "|  88|1031719.35|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Cached runtime: 0.6052045822143555 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Using the cached data, run the last query again and determine the runtime.\n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the uncached runtime.\n",
    "start_time = time.time()\n",
    "avg_price_view_cached = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales\n",
    "    GROUP BY view\n",
    "    HAVING AVG(price) >= 350000\n",
    "    ORDER BY avg_price DESC\n",
    "\"\"\")\n",
    "avg_price_view_cached.show()\n",
    "cached_runtime = time.time() - start_time\n",
    "print(f\"Cached runtime: {cached_runtime} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6a954c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data.\n",
    "df.write.mode(\"overwrite\").partitionBy(\"date_built\").parquet(\"home_sales_partitioned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6f56c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11. Read the formatted parquet data.\n",
    "# 12. Create a temporary table for the parquet data.\n",
    "parquet_df = spark.read.parquet(\"home_sales_partitioned\")\n",
    "parquet_df.createOrReplaceTempView(\"home_sales_parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "02acf96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|view| avg_price|\n",
      "+----+----------+\n",
      "|  91|1137372.73|\n",
      "|  97|1129040.15|\n",
      "|  84|1117233.13|\n",
      "|  75|1114042.94|\n",
      "|  89|1107839.15|\n",
      "|  78|1080649.37|\n",
      "|  77|1076205.56|\n",
      "|  87| 1072285.2|\n",
      "|  86|1070444.25|\n",
      "|  82| 1063498.0|\n",
      "|  90|1062654.16|\n",
      "|  99|1061201.42|\n",
      "|  76|1058802.78|\n",
      "|  85|1056336.74|\n",
      "|  95| 1054325.6|\n",
      "|  98|1053739.33|\n",
      "|  81|1053472.79|\n",
      "|  83|1033965.93|\n",
      "|  94| 1033536.2|\n",
      "|  88|1031719.35|\n",
      "+----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Parquet runtime: 0.3876931667327881 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 13. Run the last query on the parquet temporary table and determine the runtime.\n",
    "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
    "# having an average home price greater than or equal to $350,000. \n",
    "# Determine the runtime and compare it to the cached runtime.\n",
    "start_time = time.time()\n",
    "avg_price_view_parquet = spark.sql(\"\"\"\n",
    "    SELECT view, ROUND(AVG(price), 2) AS avg_price\n",
    "    FROM home_sales_parquet\n",
    "    GROUP BY view\n",
    "    HAVING AVG(price) >= 350000\n",
    "    ORDER BY avg_price DESC\n",
    "\"\"\")\n",
    "avg_price_view_parquet.show()\n",
    "parquet_runtime = time.time() - start_time\n",
    "print(f\"Parquet runtime: {parquet_runtime} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f351bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'home_sales' cached after uncache? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 14. Uncache the home_sales temporary table.\n",
    "spark.catalog.uncacheTable(\"home_sales\")\n",
    "\n",
    "# 15. Verify that the home_sales temporary table is uncached.\n",
    "print(f\"Is 'home_sales' cached after uncache? {spark.catalog.isCached('home_sales')}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Home_Sales_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
